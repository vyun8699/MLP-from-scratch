<!-- vscode-markdown-toc -->

<!-- vscode-markdown-toc-config
	numbering=true
	autoSave=true
	/vscode-markdown-toc-config -->
<!-- /vscode-markdown-toc -->
# Multilayer Perceptron (MLP) implementation with Numpy

**PROJECT BACKGROUND**

This project is part of a series of assignments for COMP5329 - Deep Learning for the Masters Degree in Data Science at The University of Sydney. 

The goal of this project is to implement a neural network without the use of modern machine learning libraries. By doing so, we will showcase the effects of different methods/components to  the overall quality of our MLP model on the provided dataset. 

The dataset was provided by the course administrators and consists of 60,000 data points in split into train (50,000) and test (10,000). Each datapoint consists of 128 floating-point numerics, split equally across 10 classes. The dataset was randomly generated by an unspecified algorithm and course participants were given free reign to define technical objective and methods.

**A. TECHNICAL OBJECTIVES**

`Definition of model quality`: we define our quality metrics as a mix of test scores, training speeds, and model simplicity. Specifically:

1. `Test scores`: we track both accuracy and F1-scores for conservatism. We note that accuracy is more appropriate for datasets which are balanced and projects with emphasis on the identification of true positives and true negatives. On the other hand, F1-score is more appropriate for datasets which are imbalanced and the ability to identify true and false positives and negatives is more important. <ins>Note that for this project, the absolute values of test scores are meaningless by themselves due to their random nature. Test scores are only useful for comparison purposes between two or more models.</ins> 

2. `Training speeds`: we focus on employing methods which are efficient with low training speeds.

3. `Simplicity`: we provide evidence-based arguments on which methods/components to implement for this specific dataset. For equivalent scores and speeds, we will opt for the methods/components which are easier to use/maintain. 

**B. EXPLORATORY DATA ANALYSIS AND PRE-PROCESSING**

<p align="center">
  <img src="assets/eda.png">
  <br>
  Dataset distribution. Notice that majority of variance are observed between index values 0 to 25.
</p>

Pre-processing applied to the dataset include:

1. `Normalization`: Train and test data were normalized using the Z-score normalisation technique where data was subtracted with the train mean and divided by train standard deviation. This transforms the train data to mean of 0 and standard deviation 1 which is appropriate for a neural network. 

2. `No dimensionality reduction is applied`: sparsity of a dataset is a feature that is useful for a deep learning network and is kept for this exercise.

3. `Train and validation split`: the original training dataset is further split into training (80%) and validation (20%) dataset. The implementation of validation checks helps with detecting overfitting during training through the implementation of early stopping (explained below). 

**C. MODULES USED IN THIS PROJECT**

Here comes the wall of text (sorry!). 

<p align="center">
  <img src="assets/mlp_sketch.png">
  <br>
  A simplified view of an MLP network.
</p>

`Base architecture`: deep learning models consists of input, hidden, and output layers through which information is propagated and processed through. A model may have one or more hidden layers and the choice of optimal number of hidden layers can be determined by considering test scores and run times. Deeper and wider models can learn more parameters from input at higher computational costs, vice versa. As information is propagated through the neural network, each neuron (e.g. a single node in the neural network) takes in one or more input multiplied by weights and adjusted by a bias factor to produce an adjusted input. The adjusted input is then further processed through a non-linear activation function to produce a convex output for that layer. The process outlined below are repeated until all training data has been ingested. A complete cycle of training loop (e.g. when all training data has been used once) is called an 'epoch':

- `Forward pass (left to right)`: (i) input data is propagated forward through a linear transformation to match the output dimension, (ii) the transformed input is then processed through the activation function to create a transformed output for the given layer, (iii) the output of each layer is then passed as input for the layer after it, which allows chaining of functions in each layer, (iv) predicted output is produced in the output layer, at the end of the forward pass. 

- `Loss calculation`: an objective function / loss function is used to calculate the degree of error between actual and predicted output. The loss function produces a larger output for a larger error, vice versa. This allows the model to compute gradients and adjust parameters to self-correct its prediction during the backward pass. The loss function used in this model is the 'criterion cross entropy', which is commonly used for multi-class classifications.

- `Backward pass (right to left)`: Errors from the predicted output can be used to calculate gradients from the output layer, all the way to the input layer via chain rule. Gradients descent is the ‘step’ taken toward the direction which minimizes the loss function. This direction of loss minimization can be calculated as the derivative of the loss function. The resulting derivative value is then multiplied to the learning rate to determine the size of step taken.

- `Update`: Weights and biases of the neurons in each layer are updated according to the gradient calculated during via gradient descent. 

`Activation functios`: are mathematical functions used in neurons which applies non-linearity to input and produces convex output. Activation functions implemented in the model are explained below:

  - `Sigmoid`: The sigmoid function is mostly used for binary classifications and probability calculations where it splits its output into values between 0 and 1 across a logistic function s-curve. 

  - `Tanh`: The tanh function resembles the sigmoid function with a range of –1 to 1. Tanh allows a wider coverage where negative inputs will be reflected.

  - `Rectified Linear Unit (‘ReLU’)`: widely used activation function in hidden layers of  neural networks. ReLU consists of two linear pieces: it defaults to zero for negative input and preserves values for positive input. ReLU is simple to implement and creates models that generalizes well. However, the tendency to create zero outputs may cause ‘dead’ neurons, whereby a neuron do not function because its input are zero.

  - `Leaky ReLU`: An alternate form to the ReLU activation function used to address ‘dead’ neurons. The leaky ReLU multiplies a small factor to negative input (instead of defaulting to zero) and preserves values for positive input. As such, the Leaky ReLU always produces some value which can be used in the proceeding layer as input, solving the ‘dead’ neuron problem. 

  - `Softmax`: The softmax function is commonly used in the output layer for multi classifications where a probability value is allocated to classes of the data. 

**D. METHODS USED IN THIS PROJECT**

To speed up the training process and increase model robustness, several methods can be used:

- `Batch size`: a higher batch size increases the training speed as it takes more than one data point in a single training epoch. However, higher batch sizes produces fewer parameter updates, which may adversely impact test scores. 
    
    - Batch size of 1 (also known as the Stochastic Gradient Descent or `SGD`): SGD takes one randomly selected data point for each iteration of the training loop. A single epoch of training requires the SGD to be until all training data is used up once (e.g. one epoch takes n number of iterations, with n being the length of the dataset).  The SGD may produce a more volatile error curve and take a very long time to converge as it calculates curves and updates parameters one-by-one, for every random data point during an epoch.

    - `Batch Gradient Descent`: On the contrary, the Batch Gradient Descent takes the entire dataset in each iteration of the training loop at once (e.g. one epoch is one iteration). This method can be useful if the data size is small and simplistic but may be cumbersome due to the large matrix multiplication done every iteration. Batch Gradient Descent produces a smoother error curve but may get stuck in local minima.

    - `Mini Batch Gradient Descent` takes a batch size value between SGD and Batch Gradient Descent.  The Mini Batch Gradient Descent is beneficial as it is customizable to match the existing computing capacity and characteristic of the dataset, thus achieving some computing benefits while maintaining some fineness of SGD. 

- `Regularization`: are techniques/methods used to avoid overfitting. Deep learning models are prone to “memorizing” patterns of dataset it is trained on, which decreases its ability to generalize when presented with an unseen dataset. Several regularization techniques used in this study: 

   - `Early stopping`: allows the model to stop training early when the model performance does not improve during evaluation. Specific to this study, early stopping is divided into two stages: (i) Stage 1: the model is trained on the training dataset and is evaluated periodically on the validation set. This stage is stopped when validation score does not improve after a specified number of cycles, (ii) Stage 2: The training process is continued on the combined training and validation datasets until the model achieves the training score at the end of Stage 1 or the maximum number of epochs is reached. Note: early stopping is used in our experiments to limit the number of epochs run and maintain computing efficiency.

<p align="center">
  <img src="assets/early_stopping.png">
  <br>
  Impact of early stopping to training loss.
</p>

   - `Weight decay`: adds a penalty to the loss function which slows down parameter updates by multiplying gradients with a gamma factor. Weight decay must be implemented with a degree of conservatism as too high a gamma value may entrap the model in a local minimum.

   - `Dropout`: removes neurons from the hidden layers based on the probability p% to ensure network robustness. By switching off nodes randomly, dropout forces the use of nodes otherwise ignored without dropout. Dropout effectively implements model parallelization which increases robustness but also the size and iteration of the model.  

   - `Momentum in SGD`: the Stochastic gradient descent has some weaknesses in deep learning whereby it may get stuck in a region where the gradients are flat (also known as 'saddle points'). The use of momentum helps our gradient descent to escape these saddle points so it is less likely to get stuck. Note: Momentum is indirectly implemented through ADAM optimizer for this exercise.

   - `Batch normalization`: as data is processed and transformed in each hidden layer, its value may not be properly fed to later layers in a deep learning model. This is known as co-variate shift. Batch normalization reparametrizes the input of each layer using z-score normalization stated above to remove the effects of covariate shift in deeper networks and it allows the full use of all nodes in every layer. Batch normalization provides net benefits in overall run-times, even if the individual epoch runtimes increase due to additional calculation involved in normalizing input for every layer. Batch normalization introduces additive and multiplicative noise which presents a regularizing effect, making dropout unnecessary. 

  <p align="center">
    <img src="assets/batchnorm.png">
    <br>
    Batch normalization alleviates the impact of covariate shift. All nodes are properly fed with information.
  </p>

   - `ADAM optimisation`: ADAM is an optimization technique created to utilise first and second moments to adapt the learning rate. Compared to the previous optimisation    technique such as RMSprop and AdaGrad where only the second moment is used, the ADAM optimisation converges quicker and develop resistance to noise in a dataset. ADAM optimizers contain the capabilities from SGD with momentum as well as RMSprop, which means it could decrease and increase the learning rate through the manipulation of the beta values depending on how large the gradient is.

**E. EXPERIMENT PROTOCOL**

`Efficient experiment protocol`: The experiment is conducted in stages to trim ‘paths’ that are unlikely to lead to a better model. In other words, we iteratively decide which parameters to tweak at every step, based on the available results. By implementing our experiment in stages, the total number of ‘paths’ explored are additive between consecutive experiments, whereas it would be multiplicative if we done exhaustively. This strategy allows us to save on computational resources and achieving acceptable runtimes. 

<p align="center">
  <img src="assets/protocol.png">
  <br>
  We only explore combination which heurestically makes sense
</p>

**F. EXPERIMENT RESULTS**

- **Experiment I: Impact of minibatch**

    In this experiment, we tested multiple values of batch sizes while keeping all other parameters constant. Duration and test scores were compared to defined performance and decide on parameters to be used in the next experiments. Results are displayed in Table 3 and Figure 3 below. batch size of 1 (which is the Stochastic Gradient Descent) is the slowest of all as it inefficiently does a forward pass, backward pass, and update for every single training data in every epoch. Batch size of 100 displays the best balance between speed and test score performance.

    Decision: batch size 100 to be used in the next experiment.

<p align="center">
  <img src="assets/experiment1.png">
  <br>
  Impact of minibatch
</p>    

- **Experiment II: Impact of deeper MLP**

    In this experiment, we tested the performance of multiple hidden layers to gauge if any improvement to the previous model can be achieved. Results are displayed in Table 4 and Figure 4 below. Results suggest that one and two hidden layers provide the best test scores at acceptable run times. There was no evidence of superiority of three hidden layers as it took longer to converge at lower test scores. The onset of overfitting can be observed starting with 2 hidden layers.

    Decision: Two hidden layers with batch size 100 to be further tested with regularization methods. One hidden layer with batch size 100 to be considered as a possible best model.

<p align="center">
  <img src="assets/experiment2.png">
  <br>
  Impact of MLP depth
</p>  

- **Experiment III: Impact of batch normalization and weight decay**

    In this experiment, the team tested the implementation of batch normalization and weight decay to regularize the model and if higher learning rates can be applied to aid run times. Results are displayed in Table 5 and Figure 5 below. From the results, batch normalization increased run-times but improved test scores while weight decay appears ineffective in most cases. The implementation of higher learning rate, batch normalization, and weight decay appears to work well and resulted in the highest test score combination among all. Implementation of batch normalisation allow the model to stabilize the features of a dataset and avoid co-variate shift within the hidden layers. However, it should be noted that the implementation of batch normalization could decrease the overall model sensitivity to weight changes. This effect reduces the efficacy of weight decay, as can be seen by the modest test score uplift in Table 5.
    
    Decision: Batch normalization to be implemented with learning rate 0.01 and without weight decay in the next experiments to maintain simplicity and faster run-time. Weight decay only provides modest test score improvements at high run time cost.

<p align="center">
  <img src="assets/experiment3.png">
  <br>
  Impact of batch normalization and weight decay
</p>  

- **Experiment IV: Impact of dropout**

    This experiment aims to explore the impact of dropout (e.g. deactivating nodes at random to build robustness) and the implementation of leaky ReLU in conjunction with dropout. Results are displayed in Table 6 and Figure 6 below. 
    
    Dropout results are worse in terms of test scores and runtimes. There are several reasons that could explain this result: 
    i.	The dataset and neural network is small/shallow. Adding dropout to a shallow model hurts performance by switching off too many relevant nodes and not letting information pass through properly,
    ii.	ReLU (which outputs zeros for negative values) is applied to a dataset that is centred around zero. This means that a large proportion of input will be zero and will not flow through the nodes (e.g. the ‘dead’ neuron problem). Adding dropout to this worsens the dead neuron problem and adversely impacts the model’s performance. 
    iii.	The team implemented leaky ReLU to alleviate this issue. Although performance increased, the implementation of dropout still produces an inferior model. 

    Decision: Dropout will not be implemented in the follow-on experiments. Leaky relu will be used as activation as it displays better combination of test scores with lower run times.

<p align="center">
  <img src="assets/experiment4.png">
  <br>
  Impact of dropout
</p>

- **Experiment V: Impact of ADAM optimization**

    This experiment tests the impact of the ADAM optimizer at different ADAM learning rates. The ADAM optimizer is an advanced method whereby it combines the effects of momentum and RMSprop in optimizing the gradient updates for weights and biases. Results are displayed in Table 7 and Figure 7 below.

    The ADAM optimizer increased the speed of convergence but did not provide noticeable improvement to test scores. We postulate that ADAM is less useful when the model is shallow and the dataset is simple. As observed in Figure 7, the loss function for ADAM starts at a higher level but goes down a steep slope after the first few epochs/iterations due to the build-up of momentum. The benefits of ADAM would be more pronounced for deeper models and larger datasets. 
    
    As discussed by Wei (2024), the ADAM optimizer might not increase performance for every circumstances. Consider the first and second moment formulas and their bias correction below:
    
    First moment (\beta_1 = 0.9)	
    m_t\ =\ \beta_1m_{t-1}+\left(1-\beta_1\right)g_t^ 
    {\hat{m}}_t\ =\ m_t\ /\left(1-\beta_1^{iter}\right)
    
    Second moment (\beta_2 = 0.99)
    v_t\ =\ \beta_2v_{t-1}+\left(1-\beta_2\right)g_t^2
    {\hat{v}}_t\ =\ v_t\ /\left(1-\beta_2^{iter}\right)
    
    
    At initialization (iter = 1), the value of  {\hat{m}}_tand {\hat{v}}_t are larger as the denominator is very small. As iter increases, the value of  {\hat{m}}_tand {\hat{v}}_t will become smaller as the denominator value approaches zero.  This results in a higher and steeper loss trajectory early in the training process, as shown in Figure 8. 
    
    As the data used in the experiment was small and other optimizations (such as mini batching and batch normalisation) were used, the benefits derived from the ADAM optimizer is minimal.
    
    Decision:  Performance of ADAM with ADAM learning rate = 0.001 and model learning rate = 0.01 to be compared with other models from the experiment.

<p align="center">
  <img src="assets/experiment5.png">
  <br>
  Impact of ADAM optimizaton
</p>

**G. RESULTS**

The strongest driver of performance in our models were the implementation of mini batch (which addresses run times) and early stopping (which addresses run times and overfitting). Variation of other parameters provided moderate benefits, but nothing as significant.

Several models with the best mix of run times, scores, and use cases are presented below. These models are the results of different stages of the experiment with representable test scores and run times.

<p align="center">
  <img src="assets/summary.png">
  <br>
  Summary of results
</p>

Recommendation: Model 1 (single hidden layer) is our best model as it has the lowest run times (2-3 times faster than other models considered), acceptable test scores (comparable to model 2) while being the simplest to implement. 

**H. DISCUSSION**

The performance of a multilayer perceptron largely depends on hyperparameter tuning and empirical testing that results from optimizing the network. In this study, a supervised learning using a multi-layer perceptron was performed. To optimize a multi-layer perceptron, the important aspects needed to be considered are (i) convergence on training data, (ii) generalisability on test data, (iii) speed and simplicity.

This study encompassed all aspects to deduce a final model most suitable for the dataset. We note that the final model may not always be the ‘fanciest’ one, but one that is fit for purpose.

In terms of convergence on training data, the dropout rate, learning rate, batch sizes, width and depth of the hidden layers played the most significant roles in affecting convergence rate. In terms of model generalisability, the usage of validation data, batch normalisation, mini-batch and learning rate contributed the most the improvement of performance on an unseen dataset.   

The system of experiments and results highlighted the importance of empirical testing and evaluation metrics on optimizing a multi-layer perceptron. Due to the adherence to a strict testing and evaluation regime, we created an adequately robust multi-layer perceptron that generalizes well in unseen dataset along with establishing the goal of understanding how different deep learning optimisation techniques could contribute to model optimisation.

**I. OTHERS**

Parameters tested are highlighted in blue. Parameters chosen after testing are in bold:

<p align="center">
  <img src="assets/appendix.png">
  <br>
  Parameters tested in blue, parameters chosen in bold
</p>

